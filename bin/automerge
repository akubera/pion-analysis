#!/usr/bin/env python3
#
# automerge
#

import re
import sys
import json
from glob import glob
from os import chdir, environ
from collections import defaultdict
import subprocess as sp
from pathlib import Path
from pprint import pprint


def arg_parser():
    from argparse import ArgumentParser
    parser = ArgumentParser()
    parser.add_argument("-n", "--dry-run",
                        action='store_true',
                        help='Do not submit job')
    parser.add_argument("masterjob",
                        nargs='+',
                        help='masterjobs to merge')
    parser.add_argument("--analysis-path",
                        default=None,
                        help='masterjobs to merge')
    return parser


def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    args = arg_parser().parse_args(argv)
    masterjobs = set(args.masterjob)

    file_paths = defaultdict(list)

    paths = []
    for jid in masterjobs:
        if re.match(r"\d{9,}", jid):
            try:
                ps_data = sp.check_output(f"gbbox ps -A -id {jid}",
                                          stderr=sp.DEVNULL,
                                          shell=True)
            except sp.CalledProcessError:
                ps_data = None
                print(f"Could not find process {jid}", file=sys.stderr)
                return 1

            if not ps_data:
                print(f"No result for {jid}, is token enabled?", file=sys.stderr)
                return 1

            paths.append(Path(ps_data.rstrip().rpartition(b' ')[2].decode()))
        else:
            paths.append(Path(jid) / 'analysis.sh')

    default_apath = "~/Physics/pion-analysis/analyses"
    apath = args.analysis_path or environ.get("FEMTOANALYSIS_PATH", default_apath)
    analysis_path = Path(apath).expanduser()

    for path, masterjob in zip(paths, masterjobs):
        job_name = path.parent.name
        job_code, _, job_id = job_name.rpartition('-')

        files = list(analysis_path.rglob(job_code))
        if len(files) == 0:
            print(f"Could not find an analysis directory for {job_name} "
                  f"(masterjob {masterjob})")
            continue
        elif len(files) > 1:
            print(f"Found multiple analyses for {job_name!r} (masterjob {masterjob})")
            continue

        file_paths[(files[0].absolute(), job_id)].append(masterjob)

    settings_cache = {}

    for (analysis_path, job_id), mjids in file_paths.items():

        if analysis_path not in settings_cache:
            with (analysis_path / 'settings.json').open() as f:
                settings_cache[analysis_path] = json.load(f)

        settings = settings_cache[analysis_path]

        # assume ordered properly
        job_idx = int(job_id) - 1
        subset_name = list(settings['data']['datasets'].keys())[job_idx]
        subset_path = (analysis_path / subset_name).absolute()
        if not (subset_path / 'analysis.root').exists():
            print("No analysis.root found. Wrong analysis directory.",
                  file=sys.stderr)
            continue

        working_dir = f"{analysis_path.name}-{job_id}"
        run_macro = (subset_path / 'RunAnalysis.C').read_text()
        if working_dir not in run_macro:
            print("Did not find job working dir in RunAnalysis macro; "
                  "probably out-of-order analyses. Skipping.",
                  file=sys.stderr)
            continue

        chdir(subset_path)
        if args.dry_run:
            print(f"Not submitting merge job in {working_dir} (dry run)")
            continue

        print(f"Running merge in {subset_path}")
        merge_args = ['aliroot', '-q', 'RunAnalysis.C', 'merge']
        aliroot_proc = sp.Popen(merge_args, stdout=sp.PIPE, stderr=sp.PIPE, encoding='utf-8')
        stdout, stderr = aliroot_proc.communicate()

        if 'Final merged results found. Not merging again.' in stdout:
            print(f"Already submitted final merge for {working_dir}")
            print(subset_path)
            continue

        m = re.search(r"Job id: '(?P<jid>\d+)'", stdout, re.M)
        if m:
            print("Merging", subset_path, "in masterjob:", m.group('jid'))
        else:
            print(aliroot_proc.returncode)
            print(f"Something went wrong processing {working_dir}, "
                  f"from masterjob {' &'.join(mjids)}")
            print(" -- stdout -- ")
            print(stdout)
            print(" -- stderr -- ")
            print(stderr)


if __name__ == "__main__":
    exit(main())
