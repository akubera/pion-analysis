#!/usr/bin/env python3
#
# fetch-train-results
#

import sys
import re
import json

from pathlib import Path
from pprint import pprint
from subprocess import getoutput
from collections import defaultdict

KV_REGEX = re.compile(r"export (?P<key>\w+)='(?P<value>[^']+)'")
DEST_DIR = Path("~/alice/data").expanduser()


DEFAULT_DATA_TRAIN = "CF_PbPb"
DEFAULT_MC_TRAIN = "CF_PbPb_MC_AOD"

def argparser():
    from argparse import ArgumentParser
    parser = ArgumentParser()
    parser.add_argument("--mc",
                        action='store_true',
                        help="Name of train to search for")
    parser.add_argument("--train",
                        nargs='?',
                        default=None,
                        help="Name of train to search for")
    parser.add_argument("--dest-dir",
                        nargs='?',
                        default='data',
                        help="Path to top-level-directory of rootfile storage")
    parser.add_argument("run_numbers",
                        nargs='*',
                        help="train numbers to pull down")
    return parser


def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    args = argparser().parse_args(argv)

    from ROOT import TGrid

    cnx = TGrid.Connect("alien://")
    pwd = cnx.Pwd()
    if not pwd:
        print("Could not connect to grid. Aborting")
        return 1

    print(f"Connected to ALICE grid. Current path is {pwd!r}")

    train = args.train or (DEFAULT_DATA_TRAIN if not args.mc else DEFAULT_MC_TRAIN)

    file_list = []

    if not args.run_numbers:
        interactive_fetch(cnx, train=train)
    else:
        print(f"Fetching {set(map(int, args.run_numbers))} from train {train!r}")

        for run in map(int, args.run_numbers):
            files = fetch_train_files(cnx, train, run)
            file_list.extend(files)

    print("\nRetrieved Files:")
    for file in file_list:
        print(file)

    return 0


def interactive_fetch(cnx, train="CF_PbPb"):
    """ Command line loop for user interactive downloading """

    def _request_train() -> int:
        nonlocal train
        train = input(f"train [{train}]: ").strip()
        number = int(input("number: "))
        return number

    while True:
        try:
            number = _request_train()
        except EOFError:
            print("")
            break
        except ValueError as e:
            print("Error:", e)
            continue
        fetch_train_files(cnx, train, number)


def fetch_train_files(cnx, train, number):
    from shlex import quote
    from shutil import copy2 as filecopy
    from ROOT import TFile

    PWGCF_train_path = '/alice/cern.ch/user/a/alitrain/PWGCF/'
    train_path = PWGCF_train_path + train

    # all files associated with this train
    found = cnx.Query(train_path, '/%d_2' % number, '', '')
    if found.GetSize() == 0:
        print("Could not find any train with number", number)
        return

    def get_local_md5(filename):
        """ Calculate md5 of local file """
        if isinstance(filename, Path):
            filename = quote(str(filename))
        local_md5, *_ = getoutput('md5sum %s' % filename).strip().split()
        return local_md5

    def fetch_grid_file(f: dict, verbose=False) -> Path:
        """ Download file if not already present """

        lfn = Path(f["lfn"])
        source_md5 = f['md5']

        local = quote(str(lfn))
        if lfn.exists():
            if verbose:
                print(f"requested file {local} exists")
            md5 = get_local_md5(lfn)
            if md5 == source_md5:
                return lfn
            print(f"md5 mismatch for {local} ({md5} â‰  {source_md5}). Overwritting.")

        lfn.parent.mkdir(exist_ok=True, parents=True)
        remote = quote(f["turl"])
        TFile.Cp(remote, local)

        md5 = get_local_md5(lfn)
        if md5 != source_md5:
            raise ValueError(f"Unexpected error: File {local} has mismatched "
                             f"md5-checksums {md5}, expected {source_md5}")
        return lfn

    def fetch_similar_files(file_list, verify_md5=True):
        """
        Searches container of file_info with same md5 value
        """

        # assert all files share md5 checksums
        if verify_md5:
            md5_set = {f['md5'] for f in file_list}
            if len(md5_set) != 1:
                raise ValueError(f"Container of files not similar (md5s: {md5_set})")

        file_paths = {Path(f['lfn']) for f in file_list}
        present_files = {f for f in file_paths if f.exists()}
        missing_files = file_paths - present_files

        # all files accounted for
        if not missing_files:
            return

        # no files present for this md5 - download one
        if not present_files:
            source_filename = missing_files.pop()
            chosen_file = next(f for f in file_list if f['lfn'] == str(source_filename))
            fetch_grid_file(chosen_file)
            present_files = {source_filename}

        # copy remaining missing files
        src_filename = str(present_files.pop())
        for dest in missing_files:
            filecopy(src_filename, str(dest))

    def parse_env_text(txt) -> dict:
        s = KV_REGEX.scanner(txt)
        env = {m.group('key'): m.group('value') for m in iter(s.search, None)}
        try:
            run_chunks = env['RUN_CHUNKS']
        except KeyError:
            pass
        else:
            from ast import literal_eval
            try:
                # Warning: eval-ing returned data
                #        : literal_eval *should* be safe
                rc = literal_eval(run_chunks)
            except SyntaxError:
                print('Unexpected format of RUN_CHUNKS:', file=sys.stderr)
                pprint(run_chunks, file=sys.stderr)
            else:
                if isinstance(rc, str):
                    rc = [rc]

                env['RUN_CHUNKS'] = [list(map(int, x.split(','))) for x in rc]

        return env

    def fetch_env_files(env_files):
        env_md5_map = {}
        for f in env_files:
            if not f['lfn'].endswith('env.sh'):
                print(f"Requested to fetch {f['lfn']} when expecting env.sh")
            env_md5_map.setdefault(f['md5'], f)

        if len(env_md5_map) != 1:
            keys = tuple(env_md5_map.keys())
            raise ValueError("Unexpected number of differing env.sh files "
                             f"for train {train}/{number} {keys!r}")
            # use this to run with differing env.sh
            for md5 in env_md5_map.keys():
                fetch_env_files({f for f in env_files if f['md5'] == md5})
            return

        env_file_info = next(iter(env_md5_map.values()))
        env_file_path = fetch_grid_file(env_file_info)
        env = parse_env_text(env_file_path.read_text())

        # copy all remaining env files - we know they are identical
        source = str(env_file_path)
        for e in env_files:
            dest = Path(e['lfn'])
            if dest.exists():
                continue
            dest.parent.mkdir(exist_ok=True, parents=True)
            filecopy(source, str(dest))

        return env_file_path, env

    # all files associated with the file
    all_files = [tmap_to_dict(f) for f in found]

    env_files = [file for file in all_files if file['lfn'].endswith('env.sh')]
    if not env_files:
        print(f"No env.sh files found for train {train}/{number}. Skipping.")
        return

    # download environment variable files and parse into dict
    try:
        env_file_path, env = fetch_env_files(env_files)
    except Exception as e:
        raise
        print(f"error fetching env.sh from {train}/{number}", file=sys.stderr)
        print(e, file=sys.stderr)
        return

    train_dir = env_file_path.parent.name

    def get_path_data(fname):
        m = re.match(r"""
                \d+_20(?P<year>\d{2})
                      (?P<month>\d{2})
                      (?P<day>\d{2})
                   .+?(?P<child>child_\d+)?$""", fname, re.X)
        return m.groupdict() if m else None

    production = env['PERIOD_NAME']
    train_id = env['TRAIN_RUN_ID']

    envpath = get_path_data(train_dir)

    dest_dir = DEST_DIR / envpath['year'] / envpath['month'] / envpath['day']
    dest_dir.mkdir(parents=True, exist_ok=True)

    prefix = f'{train}-{train_id}-{production}'

    env_json_path = dest_dir / f"{prefix}-env.json"
    env_json_path.write_text(json.dumps(env, indent=0))

    result_files = []

    # download analysis result root files
    for file_info in all_files:
        if not file_info['lfn'].endswith("AnalysisResults.root"):
            continue

        lfn = fetch_grid_file(file_info)
        merge_dir = lfn.parent
        train_dir = merge_dir.parent

        m = get_path_data(train_dir.name)
        if not m:
            print(f"Unexpected structue of train directory name {train_dir!r},"
                  " cannot create link",
                  file=sys.stderr)
            continue

        field = ('negfield' if merge_dir.name == 'merge_runlist_1' else
                 'posfield' if merge_dir.name == 'merge_runlist_2' else
                 'unknown')

        child = '-' + m['child'] if m.get('child') else ''
        link_filename = f'{prefix}{child}-{field}.root'

        link_path = dest_dir / link_filename
        if not link_path.exists():
            dest_dir.mkdir(exist_ok=True, parents=True)
            link_path.symlink_to(lfn)
        result_files.append(link_path)

    train_definition_md5s = defaultdict(list)
    for file_info in all_files:
        if file_info['lfn'].endswith("MLTrainDefinition.cfg"):
            train_definition_md5s[file_info['md5']].append(file_info)

    for file_list in train_definition_md5s.values():
        fetch_similar_files(file_list)

    return result_files

def tmap_to_dict(tmap: 'TMap') -> dict:
    from ROOT import TMapIter

    map_iter = TMapIter(tmap)
    keys = map(str, iter(map_iter, None))
    return {key: str(map_iter.Value()) for key in keys}


if __name__ == "__main__":
    sys.exit(main())
