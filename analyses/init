#!/usr/bin/env python3
#
# analyses/init
#

import os
import sys
import json
import yaml
from pathlib import Path
from shutil import copyfile


def arg_parser():
    from argparse import ArgumentParser
    parser = ArgumentParser()
    parser.add_argument('--data',
                        default='data.yaml')
    parser.add_argument('--analysis',
                        default='analysis.yaml')
    parser.add_argument('--tag',
                        help='Manually set the job tag (will be prefixed with job-)')
    parser.add_argument('--tag-suffix',
                        default='',
                        help='Add a suffix to the job tag')
    parser.add_argument('--local',
                        action='store_true',
                        help='Initialize local analysis')
    return parser


def main(argv=None):
    if argv is None:
        argv = sys.argv[1:]
    args = arg_parser().parse_args(argv)

    __dir__ = Path().absolute()

    data = yaml.safe_load((__dir__ / args.data).read_text())
    analysis = yaml.safe_load((__dir__ / args.analysis).read_text())
    macro_template = __dir__.parent.parent / 'templates' / 'RunAnalysis.C.j2'

    from alimaster.datasets import Production
    from jinja2 import Template

    macro_txt = macro_template.read_text()
    t = Template(macro_txt)

    production = Production.From(data['production'])

    vals = {
        'aliphysics_version': analysis['aliphysics-version'],
        'TTL': analysis.get('ttl', '8 * 60 * 60'),
        'production': production,
        'analysis': analysis,
    }

    if args.local:
        init_local_analysis(args)
        return

    from ROOT import TGrid

    grid = TGrid.Connect("alien://")

    from datetime import datetime
    tag = args.tag or f'{datetime.now():%y%m%d%H%M%S}'

    tag += args.tag_suffix

    local_workdir = __dir__ / "gridresults" / f'job-{tag}'
    local_workdir.mkdir(parents=True)

    upload_files = []
    for upload_file in map(Path, analysis.get('upload-files', [])):
        upload_files.append((local_workdir / upload_file).absolute())
        copyfile(upload_file, upload_files[-1])

    for i, (dset_name, runs) in enumerate(data['datasets'].items(), 1):
        hashname = production.get_hashname(runs)

        xmlfilename = production.build_xml_dataset(runs, grid=grid)
        workdir = 'job-%s-%02d' % (tag, i)
        result_filename = 'AnalysisResults-%s-%s.root' % (tag, dset_name)
        dest_wrkdir = local_workdir / dset_name
        dest_wrkdir.mkdir()

        txt = t.render(**vals,
                       xmlfile=Path(grid.GetHomeDirectory()) / 'xml' / xmlfilename,
                       workdir=workdir,
                       output_filename=result_filename)

        (dest_wrkdir / 'RunAnalysis.C').write_text(txt)

        for upload_file in upload_files:
            upload_file.link_to(dest_wrkdir / upload_file.name)

    (local_workdir / 'settings.json').write_text(json.dumps({'analysis': analysis, 'data': data}, indent=1))

    print(local_workdir)


def init_local_analysis(args):
    import os
    from pathlib import Path

    # redirect stdout
    sys.stdout.flush()
    newstdout = os.dup(1)
    devnull = os.open(os.devnull, os.O_WRONLY)
    os.dup2(devnull, 1)
    os.close(devnull)
    sys.stdout = os.fdopen(newstdout, 'w')

    from ROOT import TFile, TChain
    input_files = TChain("aodTree")

    for filename in map(str, files):
        input_files.Add(filename)


if __name__ == "__main__":
    exit(main())
